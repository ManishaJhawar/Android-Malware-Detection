import math
from collections import Counter
import numpy as np
from scipy import stats
import copy

# Class used for learning and building the Decision Tree using the given Training Set
class DecisionTree():
	dtree = {}

	def learn(self, trainData, permNames, classLabel):
		self.dtree = constructTree(trainData, permNames, classLabel)
		print ("tree = ",self.dtree)

def constructTree(trainData, permNames, classLabel):

	trainData = trainData[:]

	classLabelIndex = permNames.index(classLabel)

	classes = trainData[:, classLabelIndex]
	# print ("Classes = ",classes)
	defaultClass = stats.mode(classes)[0][0]
	# print ("Default class = ",defaultClass)

	if trainData.size==0 or (len(permNames) - 1) <= 0:
		return defaultClass
	elif (stats.mode(classes)[1][0] == len(classes)):
		return classes[0]
	else:
		best = chooseBestAttribute(permNames, classLabel, trainData)
		# print ("Best attr = ",best)
		tree = {best:{}}
    
		for val in getUniqueValues(trainData, permNames, best):
			newData = getSplittedData(best, val, trainData, permNames)
			newAttr = permNames[:]
			newAttr.remove(best)
			subtree = constructTree(newData, newAttr, classLabel)
			tree[best][val] = subtree
    
	return tree

def getSplittedData(best, val, trainData, permNames):
	bestAttrIndex = permNames.index(best)
	bestAttrCol = trainData[:, bestAttrIndex]
	newData = list()
	for i in range(len(bestAttrCol)):
		if (bestAttrCol[i] == val):
			newData.append(trainData[i])
	newData = np.delete(newData,[bestAttrIndex],axis=1)
	# print ("Length of newData[0] = ",len(newData))
	return newData


def getUniqueValues(trainData, permNames, best):
	bestAttrIndex = permNames.index(best)
	bestAttrCol = trainData[:, bestAttrIndex]
	# print ("bestAttrCol = ",bestAttrCol)
	uniqueVals = np.unique(bestAttrCol)
	# print ("uniqueVals = ",uniqueVals)
	return uniqueVals

def chooseBestAttribute(permNames, classLabel, trainData):
	gains = list()
	# print ("In chooseBestattr = ", permNames)
	for perm in permNames:
		if (perm == classLabel):
			continue
		gain = infoGain(permNames, classLabel, trainData, perm)
		# print ("Gain = ", gain)
		gains.append(gain)
	# print ("Gains = ",gains)
	maxGain = max(gains)
	maxGainIndex = gains.index(max(gains))
	return permNames[maxGainIndex]

def infoGain(permNames, classLabel, trainData, attr):
	# attrCol = Column from the trainData corressponding to the index attr
	attrIndex = permNames.index(attr)
	attrCol = trainData[:, attrIndex]
	# print ("attrCol = ",attrCol)
	freqs = Counter(attrCol)

	classLabelIndex = permNames.index(classLabel)
	classes = trainData[:, classLabelIndex]
	# print ("Classes = ",classes)

	# Separate the attrCol by the unique values present in it
	# Store the indices where the unique values occur in a dictionary
	# Mapping the class labels for each key in storeIndices
	storeIndices = {}
	for i in freqs:
		storeIndices[i] = {}
	for i in range(len(attrCol)):
		storeIndices[attrCol[i]][i] = classes[i]

	# storeIndices1 only stores the list of labels from trainClasses
	# corresspoonding to each key value in the dictionary in order
	storeOnlyLabels = {}
	for i in freqs:
		storeOnlyLabels[i] = []
	for i in range(len(attrCol)):
		storeOnlyLabels[attrCol[i]].append(classes[i])

	# Sum of each class label for each key
	'''No need of this dictionary'''
	sumLabelsForKey = {}
	for key in storeOnlyLabels:
		sumLabelsForKey[key] = {}
		a = Counter(storeOnlyLabels[key])
		for i in a:
			# print(i)
			# print(a[i])
			sumLabelsForKey[key][i] = a[i]

	# Calculating the entropy for each value the attribute/feature takes
	keys = [] # Stores each unique value the attribute/feature takes
	for i in freqs:
		keys.append(i)

	# Storing entropy for keys
	entropyForKeys = {}
	for k in keys:
		entropyForKeys[k] = calcEntropy(storeOnlyLabels[k])
	gain = 0.0
	for k in keys:
		gain += ((freqs[k]*1.0)/len(classes)) * calcEntropy(storeOnlyLabels[k])
	gain = calcEntropy(classes) - gain
	return gain

def calcEntropy(classes):
	frequency = {}
	probabilities = {}
	freqs = Counter(classes)
	for i in freqs:
		frequency[i] = freqs[i]
	for i in freqs:
		probabilities[i] = frequency[i]/len(classes)
	entropy = 0.0
	for i in probabilities:
		entropy += probabilities[i] * math.log(probabilities[i],2)
	entropy *= -1
	return entropy